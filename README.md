# Schema2QA Question Answering Dataset

## Overview
The Schema2QA dataset consists of natual questions over 6 different domains: 
restaurants, hotels, people, movies, books, and music. 
The evaluation sentences are collected from crowd workers with prompt containing 
only what the domain is, and what properties are supported. Thus the sentences are natural 
and diverse. They also contains entities unseen during training. 
The collected sentences are manually annotated with 
[ThingTalk](https://almond.stanford.edu/doc/thingtalk-intro.md) language. 

On the other hand, the training set is generated by 
[Genie](https://github.com/stanford-oval/genie-toolkit) based on crawled Schema.org metadata 
from 6 different websites. 
It's a combination of high-quality synthetic data and
a small fraction of human paraphrase data. 

## Statistics  
Train:
|                            | restaurants | people  | movies  | books   | music   | hotels  | average |
| -------------------------- | ----------- | ------- | ------- | ------- | ------- | ------- | ------- |
| \# properties supported    | 25          | 13      | 16      | 15      | 19      | 18      | 17.7    |
| Synthetic                  | 270,081     | 270,081 | 270,081 | 270,081 | 270,081 | 270,081 | 270,081 |
| Human Paraphrase           | 6,419       | 7,108   | 3,774   | 3,941   | 3,626   | 3,311   | 4,697   |
| Total (after augmentation) | 508,101     | 614,841 | 405,241 | 410,141 | 425,041 | 377,341 | 456,784 |

Evaluation: 
|            | restaurants | people | movies | books | music | hotels | average |
| ---------- | ----------- | ------ | ------ | ----- | ----- | ------ | ------- |
| Validation | 528         | 499    | 389    | 362   | 326   | 443    | 424.5   |
| Test       | 524         | 500    | 413    | 410   | 288   | 528    | 443.8   |

## Download Links
Validation data can be found under directories of each domain. 
The full training data can be found on [OVAL Wiki](https://wiki.almond.stanford.edu/releases).

## Evaluation
ThingTalk is a domain-specific langauge designed for semantic parsing task. 
A query in ThingTalk can be canonicalized. Thus, our evaluation metric is _query accuracy_, 
which considers the result to be correct only if the output is the exactly the same with 
the gold ThingTalk annotation after canonicalization. 

[Genie Toolkit](https://github.com/stanford-oval/genie-toolkit) is required to run the evaluation. 
Clone `genie-tookit` and checkout tag `emnlp2020-autoqa-camera-ready`. 
Follow its [installation guide](https://github.com/stanford-oval/genie-toolkit/blob/emnlp2020-autoqa-camera-ready/doc/install.md) 
option 1 to install it.

Clone this repository and run the following command 
```bash
# replace $(experiment) with the domain name
# replace $(name) to the name of your model (to name the result file)
# replace $(path) to the path to your model directory 
make experiment=$(experiment) model_name=$(name) model_path=$(path) evaluate
```


## Leader board 
|                                                                                           | restaurants | people | movies | books | music | hotels | average |
| ----------------------------------------------------------------------------------------- | ----------- | ------ | ------ | ----- | ----- | ------ | ------- |
| [Schema2QA](https://almond-static.stanford.edu/papers/schema2qa-cikm2020.pdf)             | 69.7%       | 75.2%  | 70.0%  | 70.0% | 63.9% | 67.0%  | 69.3%   |
| [AutoQA](https://almond-static.stanford.edu/papers/autoqa-emnlp2020.pdf) (w/o human data) | 65.3%       | 64.6%  | 66.1%  | 54.1% | 57.3% | 70.1%  | 62.9%   |

Note: A model submission page is coming soon. 
Meanwhile, please contact us at mobisocial@lists.stanford.edu to evaluate your model(s) on Schema2QA test data. 


## Updates 
An updated version of Schema2QA dataset is coming soon. 

## License
The dataset is released under [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/).
Please cite the following papers if use this dataset in your work:
```bib
% evaluation and human paraphrase data
@inproceedings{xu2020schema2qa,
  title={Schema2QA: High-Quality and Low-Cost Q\&A Agents for the Structured Web},
  author={Xu, Silei and Campagna, Giovanni and Li, Jian and Lam, Monica S},
  booktitle={Proceedings of the 29th ACM International Conference on Information \& Knowledge Management},
  pages={1685--1694},
  year={2020}
}

% auto paraphrase data
@inproceedings{xu2020autoqa,
  title={AutoQA: From Databases to Q\&A Semantic Parsers with Only Synthetic Training Data},
  author={Xu, Silei and Semnani, Sina and Campagna, Giovanni and Lam, Monica},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={422--434},
  year={2020}
}

```
